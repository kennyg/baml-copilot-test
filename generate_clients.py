#!/usr/bin/env python3
"""
Discover available GitHub Copilot models and generate BAML clients.

This script:
1. Queries GitHub Copilot API for available models
2. Tests each model to verify it works
3. Generates baml_src/clients.baml with only working models
4. Generates a test config for the test script
"""

import json
import httpx
from pathlib import Path

PROXY_URL = "http://localhost:4000"
API_KEY = "sk-baml-copilot-test"


def list_copilot_models() -> list[dict]:
    """Query available models from GitHub Copilot via LiteLLM."""
    try:
        # Query models through a github_copilot model to get the list
        response = httpx.post(
            f"{PROXY_URL}/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {API_KEY}",
                "Content-Type": "application/json",
            },
            json={
                "model": "github_copilot/gpt-4o",
                "messages": [{"role": "user", "content": "hi"}],
                "max_tokens": 1,
            },
            timeout=30,
        )
        # The models list comes from the copilot API during auth
        # For now, we'll get models from litellm's model info
    except Exception:
        pass

    # Query LiteLLM for github_copilot model info
    try:
        response = httpx.get(
            f"{PROXY_URL}/model/info",
            headers={"Authorization": f"Bearer {API_KEY}"},
            timeout=10,
        )
        if response.status_code == 200:
            data = response.json()
            return data.get("data", [])
    except Exception:
        pass

    return []


def get_copilot_models_from_api() -> list[str]:
    """Try to get model list directly from GitHub Copilot."""
    # LiteLLM exposes github copilot models - let's query what's available
    try:
        response = httpx.get(
            f"{PROXY_URL}/v1/models",
            headers={"Authorization": f"Bearer {API_KEY}"},
            timeout=10,
        )
        if response.status_code == 200:
            data = response.json()
            models = []
            for model in data.get("data", []):
                model_id = model.get("id", "")
                # Return the underlying model names for github_copilot ones
                models.append(model_id)
            return models
    except Exception:
        pass
    return []


def test_model(model_name: str) -> tuple[bool, str | None]:
    """Test if a model works and return the actual model ID."""
    try:
        response = httpx.post(
            f"{PROXY_URL}/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {API_KEY}",
                "Content-Type": "application/json",
            },
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": "Say 'ok'"}],
                "max_tokens": 5,
            },
            timeout=30,
        )

        if response.status_code == 200:
            data = response.json()
            actual_model = data.get("model", model_name)
            return True, actual_model
        return False, None
    except Exception:
        return False, None


def get_configured_models() -> list[str]:
    """Get list of models configured in the proxy."""
    try:
        response = httpx.get(
            f"{PROXY_URL}/v1/models",
            headers={"Authorization": f"Bearer {API_KEY}"},
            timeout=10,
        )
        if response.status_code == 200:
            data = response.json()
            return [m.get("id") for m in data.get("data", [])]
    except Exception:
        pass
    return []


def discover_models() -> dict[str, str]:
    """Discover which models are available on your GitHub Copilot subscription."""

    # Get models from proxy config
    configured = get_configured_models()

    if not configured:
        print("❌ No models found. Is the proxy running?")
        return {}

    print(f"Models configured in proxy ({len(configured)}):")
    for model in configured:
        print(f"  - {model}")

    print("\nTesting which models your subscription can access...\n")

    available = {}

    for alias in configured:
        print(f"  {alias}... ", end="", flush=True)
        works, actual = test_model(alias)
        if works:
            print(f"✅")
            # Map alias to underlying github_copilot model
            available[alias] = f"github_copilot/{alias}"
        else:
            print("❌")

    return available


def generate_litellm_config(available: dict[str, str]) -> None:
    """Generate litellm_config.yaml with available models."""
    config = """# LiteLLM config for GitHub Copilot
# Auto-generated by generate_clients.py

model_list:
"""
    for alias, model in available.items():
        config += f"""  - model_name: {alias}
    litellm_params:
      model: {model}
      extra_headers:
        Editor-Version: "vscode/1.95.0"
        Editor-Plugin-Version: "copilot/1.0.0"
        Copilot-Integration-Id: "vscode-chat"

"""

    config += """general_settings:
  master_key: "sk-baml-copilot-test"
"""

    Path("litellm_config.yaml").write_text(config)
    print(f"\n✅ Generated litellm_config.yaml with {len(available)} models")


def generate_baml_clients(available: dict[str, str], primary: str) -> None:
    """Generate baml_src/clients.baml with available models."""
    baml = """// GitHub Copilot clients via LiteLLM proxy
// Auto-generated by generate_clients.py - do not edit manually
// Regenerate with: mise run discover

"""
    client_names = []

    for alias, model in available.items():
        # Create a valid BAML client name
        client_name = "Copilot" + "".join(word.title() for word in alias.replace("-", " ").split())
        client_names.append((alias, client_name))

        baml += f"""client<llm> {client_name} {{
  provider "openai-generic"
  options {{
    base_url "http://localhost:4000/v1"
    model "{alias}"
    api_key env.LITELLM_API_KEY
  }}
}}

"""

    # Add CopilotDefault pointing to primary model
    primary_client = "Copilot" + "".join(word.title() for word in primary.replace("-", " ").split())
    baml += f"""// Default client - uses primary model ({primary})
client<llm> CopilotDefault {{
  provider "openai-generic"
  options {{
    base_url "http://localhost:4000/v1"
    model "{primary}"
    api_key env.LITELLM_API_KEY
  }}
}}

"""

    # Add a fallback client if we have multiple models
    if len(client_names) > 1:
        fallback_list = ", ".join(name for _, name in client_names)
        baml += f"""// Fallback strategy - try available models in order
client<llm> CopilotFallback {{
  provider "fallback"
  options {{
    strategy [{fallback_list}]
  }}
}}
"""

    Path("baml_src/clients.baml").write_text(baml)
    print(f"✅ Generated baml_src/clients.baml with {len(client_names)} clients + CopilotDefault")


def generate_test_config(available: dict[str, str]) -> None:
    """Generate a test config JSON for the test script."""
    # Pick primary model (prefer gpt-4o, then first available)
    primary = "gpt-4o" if "gpt-4o" in available else next(iter(available.keys()))

    config = {
        "primary_model": primary,
        "available_models": list(available.keys()),
        "clients": {
            alias: "Copilot" + "".join(word.title() for word in alias.replace("-", " ").split())
            for alias in available.keys()
        },
    }

    Path("test_config.json").write_text(json.dumps(config, indent=2))
    print(f"✅ Generated test_config.json (primary: {primary})")


def main():
    print("=" * 60)
    print("GitHub Copilot Model Discovery & Client Generator")
    print("=" * 60 + "\n")

    # Check proxy is running
    try:
        httpx.get(
            f"{PROXY_URL}/health",
            headers={"Authorization": f"Bearer {API_KEY}"},
            timeout=5,
        )
    except httpx.ConnectError:
        print("❌ Error: LiteLLM proxy not running at localhost:4000")
        print("   Start it with: mise run proxy")
        return 1

    # Discover available models
    available = discover_models()

    if not available:
        print("\n❌ No models available! Check your GitHub Copilot subscription.")
        return 1

    # Pick primary model (prefer gpt-4o, then first available)
    primary = "gpt-4o" if "gpt-4o" in available else next(iter(available.keys()))

    print(f"\n{'=' * 60}")
    print(f"Found {len(available)} available model(s)")
    print(f"Primary model: {primary}")
    print("=" * 60 + "\n")

    # Generate configs
    generate_litellm_config(available)
    generate_baml_clients(available, primary)
    generate_test_config(available)

    print(f"\n{'=' * 60}")
    print("Next steps:")
    print("=" * 60)
    print("  1. Restart the proxy: mise run proxy")
    print("  2. Regenerate BAML: mise run generate")
    print("  3. Run tests: mise run test")

    return 0


if __name__ == "__main__":
    exit(main())
